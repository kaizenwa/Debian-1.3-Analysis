This is Info file lang.info, produced by Makeinfo version 1.67 from the
input file lang.texi.


File: lang.info,  Node: Top,  Next: Copying,  Prev: (dir),  Up: (dir)

  This file documents the Guile `lang' module which contains tools for
writing lexical analyzers and parsers.

* Menu:

* Copying::
* Lexical Analyzers::
* Grammars::
* Parsing::


File: lang.info,  Node: Copying,  Next: Lexical Analyzers,  Prev: Top,  Up: Top

Copying
*******

                          Copyright (C) 1995

                    Free Software Foundation, Inc.

                675 Mass Ave, Cambridge, MA 02139, USA

Permission to use, copy, modify, distribute, and sell this software and
its documentation for any purpose is hereby granted without fee,
provided that the above copyright notice appear in all copies and that
both that copyright notice and this permission notice appear in
supporting documentation.

                              NO WARRANTY

BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR
THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN
OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER
EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE
ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH
YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL
NECESSARY SERVICING, REPAIR OR CORRECTION.

IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL
DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM
(INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED
INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF
THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR
OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.


File: lang.info,  Node: Lexical Analyzers,  Next: Grammars,  Prev: Copying,  Up: Top

Lexical Analyzers
*****************

     :use-module #/lang/lex

  A lexer (lexical analyzer) is an input procedure that reads lexemes:
blocks of input conforming to a specified set of patterns.  A lexer
reads from a port, either explicitly specified or the current input
port.  It consumes characters until a complete lexeme has been found.
It (normally) returns a list:

     	(<lexeme-type> <lexeme-string> ...)

  The lexeme string contains all of the characters that were read.  The
lexeme type is a symbol which describes a general category into which
the lexeme-string falls.

  An alternative behavior for a lexer is to read a complete
lexeme-string and then invoke a user-defined procedure associated with
that lexeme's type.  The procedure might cause the lexer to return a
value or might take some other action.

  A lexer can be specified as a series of regular expressions which
define the acceptable patterns of lexeme strings, and an associated
series of lexeme types or action procedures.  The tools described in
this chapter can convert a lexer specification into a lexer procedure.
They are similar too, and inspired by the series of unix programs
beginning with "lex".

* Menu:

* A Temporary Limitation::
* Performance Warning::
* Simple Examples::
* make-lexer::


File: lang.info,  Node: A Temporary Limitation,  Next: Performance Warning,  Prev: Lexical Analyzers,  Up: Lexical Analyzers

A Temporary Limitation
======================

  The lexical analyzers created by the functions in this chapter read
from ports and return tokens - but there is a temporary restriction on
what ports they can read from.

  Lexers can read only from ports returned by the function
`make-line-buffering-input-port'.  This restriction will be removed in
a future release.

     :use-module #/ice-9/lineio
     
     [This is a work-around to get past certain deficiencies in the capabilities
     of ports.  Eventually, ports should be fixed and this module nuked.]

 - Function: make-line-buffering-input-port UNDERLYING-PORT
 - Function: read-string LINE-BUFFERED-PORT
 - Function: unread-string STRING LINE-BUFFERED-PORT
     Return, read from, or push-back on a line-buffering wrapper for
     UNDERLYING-PORT.

     A line buffering input port supports:

        * `read-string' which returns the next line of input

        * `unread-string' which pushes a line back onto the stream

     Normally a "line" is all characters up to and including a newline.
     If lines are put back using unread-string, they can be broken
     arbitrarily - that is, read-string returns strings passed to
     unread-string (or shared substrings of them).


File: lang.info,  Node: Performance Warning,  Next: Simple Examples,  Prev: A Temporary Limitation,  Up: Lexical Analyzers

Performance Warning
===================

  This release is not at all performance tuned.  Lexing is usable for
some purposes, but is basically pretty slow.


File: lang.info,  Node: Simple Examples,  Next: make-lexer,  Prev: Performance Warning,  Up: Lexical Analyzers

Simple Examples
===============

  This section offers some simple examples of lexer specifications.  The
first two of these examples mirror similar examples in the manual for
Flex.

* Menu:

* Text Substitution::
* Counter::
* An Extended Example::


File: lang.info,  Node: Text Substitution,  Next: Counter,  Prev: Simple Examples,  Up: Simple Examples

Text Substitution
-----------------

  The following module defines a lexical analyzer called
`subst-username'.   It recognizes two kinds of tokens.

  One type of token is the literal string "username", for which the
defined action is to print the name field of the user's password file
entry.

  Another type of token is the "default" type, for which the defined
action is to print the token literally.

     (define-module #/scratch/example
       :use-module #/lang/lex
       :use-module #/ice-9/lineio)
     
     
     
     (define subst-username
       (let ((specification
               `(
                 ("username" ,(lambda (token port)
                                (display (vector-ref (%getpw (getuid)) 0))
                                #f))
     
                 (else       ,(lambda (token port)
                                (display token)
                                #f)))))
         (make-lexer specification)))

  Here is an example of using `subst-username'.  If you evaluate this,
it causes the lexer to read from the file ",test", processing each
lexeme until end of file is reached.  Note that we defined the actions
for all tokens to return #f.  When the end of file is reached, a non-#f
token is returned.  We use that distinction to terminate the loop:

     (with-input-from-file ",test"
       (lambda ()
         (with-input-from-port
             (make-line-buffering-input-port (current-input-port))
           (lambda ()
             (let loop ((scanner-val (subst-username)))
               (if (not scanner-val)
                   (loop (subst-username))))))))


File: lang.info,  Node: Counter,  Next: An Extended Example,  Prev: Text Substitution,  Up: Simple Examples

Counter
-------

  Here's another simple example (again mirroring the manual for Flex):

     (define line-reader
       (let ((specification
              `(
                ("[^\n]*\n"        ,(lambda (token port) token))
                ("[^\n]*"        ,(lambda (token port) token)))))
         (make-lexer specification)))

  The above specification defines a lexer that reads complete lines and
returns them in strings.

  The above lexer can be used to implement the procedure `read-line':

     (define (read-line2 port)
       (with-input-from-port
           (make-line-buffering-input-port port)
         (lambda ()
           (let ((token (line-reader)))
             (if (eof-token? token)
                 the-eof-object
                 token)))))

  The above lexer can also be used to implement the procedure that
counts the number of characters and lines in a file:

     (define (count port)
       (with-input-from-port
           (make-line-buffering-input-port port)
         (lambda ()
           (let loop ((n-chars 0)
                      (n-lines 0)
                      (next (line-reader)))
             (if (eof-token? next)
                 (list n-chars n-lines)
                 (loop (+ n-chars (string-length next))
                       (+ 1 n-lines)
                       (line-reader)))))))


File: lang.info,  Node: An Extended Example,  Prev: Counter,  Up: Simple Examples

An Extended Example
-------------------

  An extended example can be found in the source distribution that
accompanies this manual, in the file ctax/lexer.scm.   That file
defines a lexical analyzer for Ctax, a language which is explained
elsewhere in this manual (*note Ctax: (ctax)Introduction.).


File: lang.info,  Node: make-lexer,  Prev: Simple Examples,  Up: Lexical Analyzers

make-lexer
==========

  The function `make-lexer' converts lexical analyzer specifications
into lexical analyzer procedures.

 - Function: make-lexer SPECIFICATION
     Return a lexical analyzer built from SPECIFICATION.

     A lexer is built from a specification that consists of regexps and
     actions.  The regexps are listed in order of precedence, each
     matching a particular token type.

     The actions are either a token-id (a symbol or #f), or a procedure.

     If an action is a token-id, `<i>', then when a matching token is
     read, the list `(<i> <lexeme>)' is returned (where `<lexeme>' is a
     string consisting of the matched characters).

     If an action is a procedure, the procedure is called with two
     arguments - the lexeme string and the input port.  The return
     value of the procedure is returned from the lexer.

     Here is a sample specification:

          (define-public ctax-lexer-spec
            `(
              ;; Block/Statement Structuring
              ("{"                        <lbrace>)
              ("}"                        <rbrace>)
              (";"                        <semi>)
          
              ;; Defining Functions
              ("public"                   <public>)
              ("static"                   <static>)
              ("auto"                     <auto>)
          
              ;; Flow Control Keywords
              ("if"                       <if>)
              ("else"                     <else>)
              ("for"                      <for>)
              ("while"                    <while>)
              ("return"                   <return>)
              ("do"                       <do>)
              ("break"                    <break>)
              ("continue"                 <continue>)
          
              ;; Numbers
              ("[0-9]\\+\\.\\?[0-9]*"     ,(lambda (token port)
                                             (list '<number> (string->number token))))))

     The last item in the specification may optionally use the symbol
     `else' instead of regexp.  That case will cover all lexemes not
     matched by the preceeding regexps.  If not `else' case is provided,
     then unmatched lexemes return token's of type `<default>'.

     Generally speaking, given a specification, the lexer will return
     the longest matching lexeme. If two cases both match, the lexer
     will use the one that occurs first in the specification.

     The rule that longest matches may be overrided for a particular
     type of lexeme by putting the keyword :shortest after the action
     in the lexer specification.  If such a lexeme type is ever
     matched, it is returned immediately without consuming addtional
     characters to look for a longer match.


File: lang.info,  Node: Grammars,  Next: Parsing,  Prev: Lexical Analyzers,  Up: Top

Grammars
********

     :use-module #/lang/grammar

  This chapter introduces a list syntax for formal grammars and some
procedures to manipulate that syntax.  The next chapter introduces a
parsing tool that operates on grammars in this format.

* Menu:

* The Structure of Grammars::
* Production Procedures::
* Classifying Grammar Symbols::
* LR(0) NFA/DFA procedures::


File: lang.info,  Node: The Structure of Grammars,  Next: Production Procedures,  Prev: Grammars,  Up: Grammars

The Structure of Grammars
=========================

  A grammar is a list of productions.  Each production is a non-terminal
symbol, an expansion, and the body of a reduction procedure.

     ((<non-terminal> (<symbol> ...) . redux)
      (<non-terminal> (<symbol> ...) . redux)
      ...)

  All symbols, terminal and non-terminal, are represented as Scheme
symbols.

  In a book you might see:

     A --> B x Y

  or in Yacc syntax:

     A: B x Y	{ redux }

  the functions here use:

     (A (B x Y) . redux)

  Informally, the meaning of a production:

     (A (B C D E) ...)

  is: "If you want to parse an A, then parse, in order, a B, C, D, and
E".  The variable `A' is a symbol called a non-terminal.  It denotes an
abstract syntactic category like "expression" or "statement".  The
variables `B' and so on are also symbols.  Some of them may also be
non-terminals, others may be terminals.  Terminals denote abstract
syntactic categories returned by the lexical analyzer such as
"identifier" or "numeric constant".  (See ../ctax/grammar.scm for an
example grammar.)

  Productions all have an associated reduction procedure.

  `A' redux is a list of Scheme expressions, free in the variables $1,
$2, ... for up to the number of elements in the expansion.

  During a parse of a `A' according to the production

     (A (B C D) . redux)

  The parser will first parse a `B', then a `C', then a `D'.  Each
subexpression parsed returnes a value, as will the parse of `A'.  To
compute the value of parsing `A', the values of `B', `C', and `D' are
bound to $1, $2, and $3, and the redux for the `A' production is
evaluated.

  Because of the way grammars are used by the parser, productions must
not share structure (i.e., they must not contain `eq?' cons pairs).  If
you construct a grammar, but can't be certain that none of the cons
pairs in it are shared, build a usable grammar by calling `copy-tree'.


File: lang.info,  Node: Production Procedures,  Next: Classifying Grammar Symbols,  Prev: The Structure of Grammars,  Up: Grammars

Production Procedures
=====================

  As was explained in the preceeding section, a production is a list of
the form:

     (<non-terminal-symbol>
      (<expansion-symbol> <expansion-symbol>  ...)
      . <redux-body>)

 - Function: production-symbol P
     Return the symbol expanded by a production.

 - Function: production-expansion P
     Return the list of symbols in the expansion of a production.

 - Function: production-body P
     Return the body of a productions reduction procedure.

 - Function: symbol-productions G NT
     Return the productions in grammar G that expand symbol NT.


File: lang.info,  Node: Classifying Grammar Symbols,  Next: LR(0) NFA/DFA procedures,  Prev: Production Procedures,  Up: Grammars

Classifying Grammar Symbols
===========================

  The "grammar symbols" of a grammar `g' are all symbols that occur in
the expansion of a production in `g' or as the production symbol of a
production in `g'.

  Grammar symbols can be further divided into various categories:

 - Function: grammar-start-symbol G
     Return the starting symbol of G.

     By convention, the first production determines the start symbol
     but in fact, any symbol could be used.   A grammar can have
     mulitple entry points, for example.


File: lang.info,  Node: LR(0) NFA/DFA procedures,  Prev: Classifying Grammar Symbols,  Up: Grammars

LR(0) NFA/DFA procedures
========================

     :use-module lr0.scm

  [N.B. If all you want to do is parse, this section is particularly
irrelevant.  But if you want to hack parsing tools, this section may be
useful.]

  A grammar implies a set of "grammar items" which can be interpreted
as states in an NFA.  An item corresponds to a choice of one production
from the grammar, and a position within that production.

  For example, if a production is:

     A -> B C D                      aka     (a (b c d) . <reduction rule>)

  Then the items are:

     A -> * B C D           aka     ((a (b c d) . <reduction rule>) b c d)
     A -> B * C D           aka     ((a (b c d) . <reduction rule>) c d)
     A -> B C * D           aka     ((a (b c d) . <reduction rule>) d)
     A -> B C D *           aka     ((a (b c d) . <reduction rule>))

  An empty production has exactly one item:

     A -> *                 aka     ((a () . <reduction rule>))

  If an item has nothing to the left of the star, it is called an
"initial item".

  If an item has nothing to the right of the star, it is called an
"final item".

  An item says what we think we are parsing (an `A', in the above
examples) and what we expect the next bit of input to be (for example,
in the item `A -> * B C D', we next expect to parse a `B').

  These items are states in an NFA, the LR(0) NFA, with edges defined
between them.

  The first item in the grammar is conventionally the start state,
although any initial item can be used for this purpose.

  Final items are final NFA states.  A final state implies that an
entire expansion has been seen and now a reduction of that expansion is
possible.  Final states have no outward edges.

  An NFA state (item) has two kinds of edges corresponding to whether
the symbol to the right of the star is a terminal or non-terminal.

  Terminal edges indicate that a particular type of input token is
expected.  If the lookahead is of that type, it can be shifted onto the
parse stack whie crossing the terminal edge to a new state.

  Non-terminal edges indicate a point where a nested construct should be
parsed.  If in state I the symbol following the star is non-terminal
`B', then there is an epsilon edge to all initial items associated with
productions of `B'.  In case one of those epsilon edges leads to a
correct parse, there is also an edge labeled `B' that can be used to
shift the `B' generated by a reduction.

  For example, in the state:

     A -> w * B u

  it is legal to shift the non-terminal `B' (this is sometimes called a
"goto" move).  It is also legal to make an epsilon transition to states:

     B -> * w
     B -> * u
     ...

  presuming the corresponding productions exist.  Informally: you're
allowed to absorb the next burst of input as a reduction to `B',
therefore, you are also allowed at this point to begin working on a
parse of `B'.

  The rule applies transitively so that if `A -> w * B u' epsilon
transitions to `B -> * C v', that in turn epsilon transitions to `C ->
* z'.

  To summarize: The lr(0) NFA has final states which specify reductions,
terminal transitions, which specify permissible input, non-terminal
transitions, which specify permissible feedback from the parsing of
nested constructs, and epsilon edges, which specify points at which
nested constructs may begin.

  The above describes all the states, designates the start and final
states and defines the transition function of a non-deterministic
automata.  The epsilon edges associated with non-terminal transitions
prevent the automata from being deterministic.

  But the lr(1) parser requires a deterministic lr(0) automata So the
code in this file not only must construct the lr(0) NFA, but it must
convert that NFA into a DFA.

  To parse deterministically, we apply a few stern measures.  First, we
combine all states accessible by epsilon transitions into superstates.
Second, when components of a superstate disagree about whether to
shift some input or reduce because of it, we favor shifting.   Third,
we only permit reductions if we can guarantee that after a series of
reductions, the look-ahead token will be shifted.   Fourth, if the
components of a superstate disagree about which reduction rule to
apply, we pick whichever one we discover first (clearly this could be
improved). Fifth and finally, if only some components of a superstate
think the look-ahead token is an error, we ignore them (if all
components agree the look-ahead is an error, then so do we).

  Its useful to prove that a particular grammar has the property that
components of an achievable superstate will never disagree about what
reduction rule to apply.  This can be proven automatically.  One
convenient technique for lalr(1) grammars is to convert them to yacc
syntax and run a table generator on them.  Some day, grammar debugging
tools will be added to this library.

  In the usual way, we can construct a DFA from an LR(0) NFA, combining
multiple NFA states accessible along epsilon paths into singular DFA
superstates.

 - Function: item-set-start-kernel G
     Return the kernel of the starting superstate.

     Only the initial items of start-symbol productions are included in
     this set.   Compose with `item-set-closure' to also obtain all
     items reachable by epsilon transitions.

 - Function: item-set-successor-kernel G IS S
     Return `is'', the state that follows IS after shifting S.

     `#f' is returned if S can not be shifted from state IS.

     Only the items which are direct successors of items in IS are
     present in the successor kernel.  To also include all items
     reachable by epsilon transitions, compose this function with
     `item-set-closure'.

 - Function: item-set-closure G IS
     Return the epsilon transition closure of IS.

     In the LR(0) NFA, if we are at the state:

          A -> w * B v

     Then we are also an epsilon transition (not to be confused with an
     empty reduction) away from also being in the items:

          B -> * u
          B -> * r
          B -> * s
          ...

     So if a superstate contains `A-> w * B v', then it should also
     contain the inital states of `B'.  This rule applies transitively.

     If IS was returned by `item-set-start-kernel' then this procedure
     returns the DFA start state.  If IS was returned by
     ITEM-SET-SUCCESSOR-KERNEL, and the set argument to
     ITEM-SET-SUCCESSOR-KERNEL was a DFA state, then this procedure
     returns a DFA state.

 - Function: item-set-reductions G IS
     Return the set of all productions that can be reduced from IS.
     Context and look-ahead are disregarded.

  Items and items sets are an abstract data type whose representation is
hidden from the definitions of the five basic superstate functions,
`item-set-successor', `item-set-start-kernel', `item-set-reductions',
`item-set-closure', and `item-set-nullable?'.

  The abstract data type is encapsulated in the definitions listed
below:

 - Function: make-item G PRODUCTION RIGHT-HAND-EXPANSION
     Return an item for production with the * before
     RIGHT-HAND-EXPANSION.  RIGHT-HAND-EXPANSION must be eq? to some
     tail of the expansion part of PRODUCTION.

 - Function: item-production G ITEM
     Return the production corresponding to ITEM

 - Function: item-position G ITEM
     Return the right-hand-expansion corresponding to ITEM.

 - Function: item-set G . ITEMS
     Construct an item set from the item arguments.

 - Function: item-set-map G F IS
     Call F repeatedly, passing each item in IS, returning a list of
     results.

 - Function: pick-item-set-mappings G F IS
     Call F repeatedly, passing each item of IS, returning a list of
     the non-#f results.

 - Function: item-set-union G ISA ISB ISC ...
 - Function: item-set-empty? G IS
     Standard set operations on item sets.

 - Function: grammar-non-terminals G
     Return the non-terminal symbols of G.

     The list is computed by looking at the production symbol of each
     production.

 - Function: grammar-terminals G
     Return the list of terminals of G.

     The list is computed by looking at all the symbols used in
     expansions, and subtracting out the non-terminals.

 - Function: grammar-nullables G
     Return those non-terminals of G that can derive the empty string.


File: lang.info,  Node: Parsing,  Prev: Grammars,  Up: Top

Parsing
*******

  This chapter is about how to parse using the kinds of grammar
described by the preceeding chapter.

* Menu:

* Calling `parser'::
* Parsing Overview::
* Parsing in Detail::


File: lang.info,  Node: Calling `parser',  Next: Parsing Overview,  Prev: Parsing,  Up: Parsing

Calling `parser'
================

 - Function: parser G LEXER ERROR MODULE
     Parse input according to the grammar G.

     LEXER should be a thunk that returns successive input tokens.

     ERROR is a procedure that is called in responce to parse errors.
     It is called with the arguments:

          (error continue-parser-loop state-stack value-stack token lexer)

     The error routine can exit non-locally or return immediately (to
     the caller of `parser').  It can also invoke
     `continue-parser-loop' this way:

          (continue-parser-loop new-state-stack new-value-stack new-token new-lexer new-error)

     to resume the parse.

     MODULE is the module in which to evaluate reduction procedures.

  These procedures operate on value stacks:

 - Function: parser-value-cons VAL STACK
 - Function: parser-value-car STACK
 - Function: parser-value-cdr STACK
 - Function: parser-value-cdr-ref STACK
     Standard stack operations for value stacks.

  These procedures operate on state stacks:

 - Function: parser-context-cons STATE STACK
 - Function: parser-context-car STACK
 - Function: parser-context-cdr STACK
 - Function: parser-context-cdr-ref STACK
     Standard stack operations for state stacks.

 - Function: parser-start-state G
     Return the parser starting state for grammar G.

 - Function: make-parser-state G IS
 - Function: parser-state-item-set STATE
     Return the parser state corresponding to an lr(0) item set (and
     vice versa).

 - Function: parser-action G STATE TOKEN-TYPE MODULE
     Return a parser action for STATE with a TOKEN-TYPE look-ahead.

     The return value is a list matching one of these patterns:

          ;; A shift action.   The argument is the parser state after the shift.
          ;;
          `(shift ,shift)
          
          ;; A reduce action.  The argument is a list of reduction procedures.
          ;;
          `(reduce . ,funcs)
          
          ;; A parser error -- no action is defined.
          ;;
          `(error)

  A reduce action is a procedure returned by `production-function'.

 - Function: production-function G PRODUCTION MODULE
     Return a reduction function for PRODUCTION.

     The production contains expressions that are free in the variables
     `$1', `$2' etc.   `production-function' turns these expressions
     into Scheme procedures.

     Its probably not important to know how to call a reduction
     function - this is something that only the parser itself should
     ever do.


File: lang.info,  Node: Parsing Overview,  Next: Parsing in Detail,  Prev: Calling `parser',  Up: Parsing

Parsing Overview
================

  The procedure `parser' implements a conventional shift-reduce parser
with one look-ahead token.  The parser has, however, some unsual
properties:

   * *No reductions without shifting.*

     Suppose the look-ahead can not be shifted, but that reductions
     (possilby more than one) are possible.  `parser' will not perform
     any reduction until it finds a series of reductions that will
     allow the look-ahead token to be shifted.  If no such series of
     reductions exists, than an error is signaled immediately.

     A consequence of this property is that reduce/reduce conflicts in a
     grammar are not necessarily bugs - they may simply be points in a
     grammar where the parser has to do a backtracking search through
     the possible reductions to find a way to advance the parse.

   * *Parsing Never Ends*

     Parsing continues forever unless a reduction rule exits non-locally
     (by invoking a continuation or `throw').


File: lang.info,  Node: Parsing in Detail,  Prev: Parsing Overview,  Up: Parsing

Parsing in Detail
=================

  The comments in lang/lr1.scm describe the parsing algorithm in greater
detail.



Tag Table:
Node: Top95
Node: Copying348
Node: Lexical Analyzers2153
Node: A Temporary Limitation3527
Node: Performance Warning4898
Node: Simple Examples5181
Node: Text Substitution5546
Node: Counter7266
Node: An Extended Example8710
Node: make-lexer9096
Node: Grammars11976
Node: The Structure of Grammars12437
Node: Production Procedures14468
Node: Classifying Grammar Symbols15211
Node: LR(0) NFA/DFA procedures15880
Node: Parsing24318
Node: Calling `parser'24573
Node: Parsing Overview27185
Node: Parsing in Detail28284

End Tag Table
