\input texinfo @c -*-texinfo-*-
@c %**start of header
@setfilename lang.info
@settitle Language Implemtation Tools
@setchapternewpage on
@c Choices for setchapternewpage are {on,off,odd}.
@paragraphindent 2
@c %**end of header

@iftex
@finalout
@c DL: lose the egregious vertical whitespace, esp. around examples
@c but paras in @defun-like things don't have parindent
@parskip 4pt plus 1pt
@end iftex

@titlepage
@title Language Implemtation Tools
@author by Tom Lord

@page
@vskip 0pt plus 1filll
Copyright @copyright{} 1995 Free Software Foundation, Inc.

Permission is granted to make and distribute verbatim copies of
this manual provided the copyright notice and this permission notice
are preserved on all copies.

Permission is granted to copy and distribute modified versions of this
manual under the conditions for verbatim copying, provided that the entire
resulting derived work is distributed under the terms of a permission
notice identical to this one.

Permission is granted to copy and distribute translations of this manual
into another language, under the above conditions for modified versions,
except that this permission notice may be stated in a translation approved
by the author.
@end titlepage

@node Top, Copying, (dir), (dir)

@ifinfo

This file documents the Guile @code{lang} module which contains tools
for writing lexical analyzers and parsers.

@end ifinfo

@menu
* Copying::                     
* Lexical Analyzers::           
* Grammars::                    
* Parsing::                     
@end menu

@node Copying, Lexical Analyzers, Top, Top
@chapter Copying

@center Copyright (C) 1995
@center Free Software Foundation, Inc.
@center 675 Mass Ave, Cambridge, MA 02139, USA

@noindent
Permission to use, copy, modify, distribute, and sell this software and
its documentation for any purpose is hereby granted without fee,
provided that the above copyright notice appear in all copies and that
both that copyright notice and this permission notice appear in
supporting documentation.

@center NO WARRANTY

@noindent
BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR
THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN
OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER
EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE
ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH
YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL
NECESSARY SERVICING, REPAIR OR CORRECTION.

@noindent
IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL
DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM
(INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED
INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF
THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR
OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.

@node  Lexical Analyzers, Grammars, Copying, Top
@chapter Lexical Analyzers


@example
:use-module #/lang/lex
@end example


A lexer (lexical analyzer) is an input procedure that reads lexemes:
blocks of input conforming to a specified set of patterns.  A lexer
reads from a port, either explicitly specified or the current input
port.  It consumes characters until a complete lexeme has been found.
It (normally) returns a list:

@example
	(<lexeme-type> <lexeme-string> ...)
@end example

The lexeme string contains all of the characters that were read.  The
lexeme type is a symbol which describes a general category into which
the lexeme-string falls.

An alternative behavior for a lexer is to read a complete lexeme-string
and then invoke a user-defined procedure associated with that lexeme's
type.  The procedure might cause the lexer to return a value or might
take some other action.

A lexer can be specified as a series of regular expressions which define
the acceptable patterns of lexeme strings, and an associated series of
lexeme types or action procedures.  The tools described in this chapter
can convert a lexer specification into a lexer procedure.  They are
similar too, and inspired by the series of unix programs beginning with
"lex".


@menu
* A Temporary Limitation::      
* Performance Warning::         
* Simple Examples::             
* make-lexer::                  
@end menu

@node  A Temporary Limitation, Performance Warning, Lexical Analyzers, Lexical Analyzers
@section A Temporary Limitation

The lexical analyzers created by the functions in this chapter read from
ports and return tokens -- but there is a temporary restriction on what
ports they can read from.

Lexers can read only from ports returned by the function
@code{make-line-buffering-input-port}.  This restriction will be
removed in a future release.

@example
:use-module #/ice-9/lineio

[This is a work-around to get past certain deficiencies in the capabilities
of ports.  Eventually, ports should be fixed and this module nuked.]
@end example

@defun make-line-buffering-input-port underlying-port
@defunx read-string line-buffered-port
@defunx unread-string string line-buffered-port

Return, read from, or push-back on a line-buffering wrapper for @var{underlying-port}.

A line buffering input port supports:

@itemize @bullet{}
@item @code{read-string}
which returns the next line of input
@item @code{unread-string}
which pushes a line back onto the stream
@end itemize

Normally a "line" is all characters up to and including a newline.  If
lines are put back using unread-string, they can be broken arbitrarily
-- that is, read-string returns strings passed to unread-string (or
shared substrings of them).
@end defun



@node  Performance Warning, Simple Examples, A Temporary Limitation, Lexical Analyzers
@section Performance Warning

This release is not at all performance tuned.  Lexing is usable for some
purposes, but is basically pretty slow.

@node  Simple Examples, make-lexer, Performance Warning, Lexical Analyzers
@section Simple Examples

This section offers some simple examples of lexer specifications.  The
first two of these examples mirror similar examples in the manual for
Flex.

@menu
* Text Substitution::           
* Counter::                     
* An Extended Example::         
@end menu


@node  Text Substitution, Counter, Simple Examples, Simple Examples
@subsection Text Substitution

The following module defines a lexical analyzer called
@code{subst-username}.   It recognizes two kinds of tokens.

One type of token is the literal string "username", for which the
defined action is to print the name field of the user's password
file entry.

Another type of token is the "default" type, for which the defined
action is to print the token literally.


@example
(define-module #/scratch/example
  :use-module #/lang/lex
  :use-module #/ice-9/lineio)



(define subst-username
  (let ((specification
          `(
            ("username" ,(lambda (token port)
                           (display (vector-ref (%getpw (getuid)) 0))
                           #f))

            (else       ,(lambda (token port)
                           (display token)
                           #f)))))
    (make-lexer specification)))
@end example

Here is an example of using @code{subst-username}.  If you evaluate
this, it causes the lexer to read from the file ",test", processing each
lexeme until end of file is reached.  Note that we defined the actions
for all tokens to return #f.  When the end of file is reached, a non-#f
token is returned.  We use that distinction to terminate the loop:

@example
(with-input-from-file ",test"
  (lambda () 
    (with-input-from-port
        (make-line-buffering-input-port (current-input-port))
      (lambda () 
        (let loop ((scanner-val (subst-username)))
          (if (not scanner-val)
              (loop (subst-username))))))))
@end example



@node  Counter, An Extended Example, Text Substitution, Simple Examples
@subsection Counter



Here's another simple example (again mirroring the manual for Flex):

@example
(define line-reader
  (let ((specification
         `(
           ("[^\n]*\n"        ,(lambda (token port) token))
           ("[^\n]*"        ,(lambda (token port) token)))))
    (make-lexer specification)))
@end example


The above specification defines a lexer that reads complete lines and
returns them in strings.

The above lexer can be used to implement the procedure @code{read-line}:

@example
(define (read-line2 port)
  (with-input-from-port
      (make-line-buffering-input-port port)
    (lambda ()
      (let ((token (line-reader)))
        (if (eof-token? token)
            the-eof-object
            token)))))
@end example

The above lexer can also be used to implement the procedure that counts
the number of characters and lines in a file:


@example
(define (count port)
  (with-input-from-port
      (make-line-buffering-input-port port)
    (lambda ()
      (let loop ((n-chars 0)
                 (n-lines 0)
                 (next (line-reader)))
        (if (eof-token? next)
            (list n-chars n-lines)
            (loop (+ n-chars (string-length next))
                  (+ 1 n-lines)
                  (line-reader)))))))
@end example

@node  An Extended Example,  , Counter, Simple Examples
@subsection An Extended Example

An extended example can be found in the source distribution that
accompanies this manual, in the file ctax/lexer.scm.   That file
defines a lexical analyzer for Ctax, a language which is explained
elsewhere in this manual (@pxref{Introduction, Ctax, Introduction, ctax, Ctax Language Implementation Tools}).


@node  make-lexer,  , Simple Examples, Lexical Analyzers
@section make-lexer

The function @code{make-lexer} converts lexical analyzer specifications
into lexical analyzer procedures.

@defun make-lexer specification

Return a lexical analyzer built from @var{specification}.

A lexer is built from a specification that consists of regexps and
actions.  The regexps are listed in order of precedence, each matching a
particular token type.

The actions are either a token-id (a symbol or #f), or a procedure.

If an action is a token-id, @code{<i>}, then when a matching token is
read, the list @code{(<i> <lexeme>)} is returned (where @code{<lexeme>}
is a string consisting of the matched characters).

If an action is a procedure, the procedure is called with two arguments
-- the lexeme string and the input port.  The return value of the
procedure is returned from the lexer.

Here is a sample specification:

@example
(define-public ctax-lexer-spec
  `(
    ;; Block/Statement Structuring
    ("@{"                        <lbrace>)
    ("@}"                        <rbrace>)
    (";"                        <semi>)

    ;; Defining Functions
    ("public"                   <public>)
    ("static"                   <static>)
    ("auto"                     <auto>)

    ;; Flow Control Keywords
    ("if"                       <if>)
    ("else"                     <else>)
    ("for"                      <for>)
    ("while"                    <while>)
    ("return"                   <return>)
    ("do"                       <do>)
    ("break"                    <break>)
    ("continue"                 <continue>)

    ;; Numbers
    ("[0-9]\\+\\.\\?[0-9]*"     ,(lambda (token port)
                                   (list '<number> (string->number token))))))
@end example


The last item in the specification may optionally use the symbol
@code{else} instead of regexp.  That case will cover all lexemes not
matched by the preceeding regexps.  If not @code{else} case is provided,
then unmatched lexemes return token's of type @code{<default>}.

Generally speaking, given a specification, the lexer will return the
longest matching lexeme. If two cases both match, the lexer will use the
one that occurs first in the specification.

The rule that longest matches may be overrided for a particular type of
lexeme by putting the keyword :shortest after the action in the lexer
specification.  If such a lexeme type is ever matched, it is returned
immediately without consuming addtional characters to look for a longer
match.
@end defun


@node  Grammars, Parsing, Lexical Analyzers, Top
@chapter Grammars

@example
:use-module #/lang/grammar
@end example

This chapter introduces a list syntax for formal grammars
and some procedures to manipulate that syntax.  The next
chapter introduces a parsing tool that operates on grammars
in this format.


@menu
* The Structure of Grammars::   
* Production Procedures::       
* Classifying Grammar Symbols::  
* LR(0) NFA/DFA procedures::    
@end menu

@node  The Structure of Grammars, Production Procedures, Grammars, Grammars
@section The Structure of Grammars

A grammar is a list of productions.  Each production is a non-terminal
symbol, an expansion, and the body of a reduction procedure.

@example
((<non-terminal> (<symbol> ...) . redux)
 (<non-terminal> (<symbol> ...) . redux)
 ...)
@end example

All symbols, terminal and non-terminal, are represented as 
Scheme symbols.

In a book you might see:

@example
A --> B x Y
@end example

or in Yacc syntax:

@example
A: B x Y	@{ redux @}
@end example

the functions here use:

@example
(A (B x Y) . redux)
@end example

Informally, the meaning of a production:

@example
(A (B C D E) ...)
@end example

is: "If you want to parse an A, then parse, in order, a B, C, D, and E".
The variable @code{A} is a symbol called a non-terminal.  It denotes an
abstract syntactic category like "expression" or "statement".  The
variables @code{B} and so on are also symbols.  Some of them may also be
non-terminals, others may be terminals.  Terminals denote abstract
syntactic categories returned by the lexical analyzer such as
"identifier" or "numeric constant".  (See ../ctax/grammar.scm for an
example grammar.)

Productions all have an associated reduction procedure.

@code{A} redux is a list of Scheme expressions, free in the variables
$1, $2, ... for up to the number of elements in the expansion.

During a parse of a @code{A} according to the production

@example
(A (B C D) . redux)
@end example


The parser will first parse a @code{B}, then a @code{C}, then a
@code{D}.  Each subexpression parsed returnes a value, as will the parse
of @code{A}.  To compute the value of parsing @code{A}, the values of
@code{B}, @code{C}, and @code{D} are bound to $1, $2, and $3, and the
redux for the @code{A} production is evaluated.

Because of the way grammars are used by the parser, productions must not
share structure (i.e., they must not contain @code{eq?} cons pairs).  If
you construct a grammar, but can't be certain that none of the cons
pairs in it are shared, build a usable grammar by calling
@code{copy-tree}.


@node  Production Procedures, Classifying Grammar Symbols, The Structure of Grammars, Grammars
@section Production Procedures

As was explained in the preceeding section, a production is a list of
the form:

@example
(<non-terminal-symbol>
 (<expansion-symbol> <expansion-symbol>  ...)
 . <redux-body>)
@end example


@defun production-symbol p
Return the symbol expanded by a production.
@end defun

@defun production-expansion p
Return the list of symbols in the expansion of a production.
@end defun

@defun production-body p
Return the body of a productions reduction procedure.
@end defun

@defun symbol-productions g nt
Return the productions in grammar @var{g} that expand symbol @var{nt}.
@end defun



@node  Classifying Grammar Symbols, LR(0) NFA/DFA procedures, Production Procedures, Grammars
@section Classifying Grammar Symbols

The @dfn{grammar symbols} of a grammar @code{g} are all symbols
that occur in the expansion of a production in @code{g} or as the
production symbol of a production in @code{g}.

Grammar symbols can be further divided into various categories:

@defun grammar-start-symbol g
Return the starting symbol of @var{g}.

By convention, the first production determines the start symbol
but in fact, any symbol could be used.   A grammar can have
mulitple entry points, for example.
@end defun


@node  LR(0) NFA/DFA procedures,  , Classifying Grammar Symbols, Grammars
@section LR(0) NFA/DFA procedures

@example
:use-module lr0.scm
@end example

[N.B. If all you want to do is parse, this section is particularly
irrelevant.  But if you want to hack parsing tools, this section
may be useful.]

A grammar implies a set of @dfn{grammar items} which can be interpreted
as states in an NFA.  An item corresponds to a choice of one production
from the grammar, and a position within that production.

For example, if a production is:

@example
A -> B C D                      aka     (a (b c d) . <reduction rule>)
@end example

Then the items are:

@example
A -> * B C D           aka     ((a (b c d) . <reduction rule>) b c d)
A -> B * C D           aka     ((a (b c d) . <reduction rule>) c d)
A -> B C * D           aka     ((a (b c d) . <reduction rule>) d)
A -> B C D *           aka     ((a (b c d) . <reduction rule>))
@end example

An empty production has exactly one item:

@example
A -> *                 aka     ((a () . <reduction rule>))
@end example

If an item has nothing to the left of the star, it is called an
@dfn{initial item}.

If an item has nothing to the right of the star, it is called an
@dfn{final item}.

An item says what we think we are parsing (an @code{A}, in the above
examples) and what we expect the next bit of input to be (for example,
in the item @code{A -> * B C D}, we next expect to parse a @code{B}).

These items are states in an NFA, the LR(0) NFA, with edges defined
between them.

The first item in the grammar is conventionally the start state,
although any initial item can be used for this purpose.

Final items are final NFA states.  A final state implies that an entire
expansion has been seen and now a reduction of that expansion is
possible.  Final states have no outward edges.

An NFA state (item) has two kinds of edges corresponding to whether the
symbol to the right of the star is a terminal or non-terminal.

Terminal edges indicate that a particular type of input token is
expected.  If the lookahead is of that type, it can be shifted onto the
parse stack whie crossing the terminal edge to a new state.

Non-terminal edges indicate a point where a nested construct should be
parsed.  If in state I the symbol following the star is non-terminal
@code{B}, then there is an epsilon edge to all initial items associated
with productions of @code{B}.  In case one of those epsilon edges leads
to a correct parse, there is also an edge labeled @code{B} that can be
used to shift the @code{B} generated by a reduction.

For example, in the state:

@example
A -> w * B u
@end example

it is legal to shift the non-terminal @code{B} (this is sometimes called a "goto" move). 
It is also legal to make an epsilon transition to states:

@example
B -> * w
B -> * u
...
@end example

presuming the corresponding productions exist.  Informally: you're
allowed to absorb the next burst of input as a reduction to @code{B},
therefore, you are also allowed at this point to begin working on
a parse of @code{B}.

The rule applies transitively so that if @code{A -> w * B u} epsilon
transitions to @code{B -> * C v}, that in turn epsilon transitions
to @code{C -> * z}.

To summarize: The lr(0) NFA has final states which specify reductions,
terminal transitions, which specify permissible input, non-terminal
transitions, which specify permissible feedback from the parsing of
nested constructs, and epsilon edges, which specify points at which
nested constructs may begin.

The above describes all the states, designates the start and final
states and defines the transition function of a non-deterministic
automata.  The epsilon edges associated with non-terminal transitions
prevent the automata from being deterministic.

But the lr(1) parser requires a deterministic lr(0) automata
So the code in this file not only must construct the lr(0) NFA,
but it must convert that NFA into a DFA.

To parse deterministically, we apply a few stern measures.
First, we combine all states accessible by epsilon transitions
into superstates.   Second, when components of a superstate disagree
about whether to shift some input or reduce because of it, we favor
shifting.   Third, we only permit reductions if we can guarantee
that after a series of reductions, the look-ahead token will be 
shifted.   Fourth, if the components of a superstate disagree 
about which reduction rule to apply, we pick whichever one we
discover first (clearly this could be improved). Fifth and finally,
if only some components of a superstate think the look-ahead 
token is an error, we ignore them (if all components agree the look-ahead
is an error, then so do we).

Its useful to prove that a particular grammar has the property that
components of an achievable superstate will never disagree about
what reduction rule to apply.  This can be proven automatically.
One convenient technique for lalr(1) grammars is to convert them 
to yacc syntax and run a table generator on them.  Some day, grammar
debugging tools will be added to this library.


In the usual way, we can construct a DFA from an LR(0) NFA, combining
multiple NFA states accessible along epsilon paths into singular DFA
superstates.

@defun item-set-start-kernel g
Return the kernel of the starting superstate.

Only the initial items of start-symbol productions
are included in this set.   Compose with @code{item-set-closure} 
to also obtain all items reachable by epsilon transitions.
@end defun


@defun item-set-successor-kernel g is s
Return @code{is'}, the state that follows @var{is} after shifting @var{s}.

@code{#f} is returned if @var{s} can not be shifted from state @var{is}.

Only the items which are direct successors of items in @var{is} are
present in the successor kernel.  To also include all items reachable by
epsilon transitions, compose this function with @code{item-set-closure}.
@end defun


@defun item-set-closure g is
Return the epsilon transition closure of @var{is}.

In the LR(0) NFA, if we are at the state:

@example
A -> w * B v
@end example

Then we are also an epsilon transition (not to be confused
with an empty reduction) away from also being in the items:

@example
B -> * u
B -> * r
B -> * s
...
@end example

So if a superstate contains @code{A-> w * B v}, then it should also
contain the inital states of @code{B}.  This rule applies transitively.

If @var{is} was returned by @code{item-set-start-kernel} then this
procedure returns the DFA start state.  If @var{is} was returned by
@var{item-set-successor-kernel}, and the set argument to
@var{item-set-successor-kernel} was a DFA state, then this procedure
returns a DFA state.
@end defun


@defun item-set-reductions g is
Return the set of all productions that can be reduced from @var{is}.
Context and look-ahead are disregarded.
@end defun

Items and items sets are an abstract data type whose representation is
hidden from the definitions of the five basic superstate functions,
@code{item-set-successor}, @code{item-set-start-kernel},
@code{item-set-reductions}, @code{item-set-closure}, and
@code{item-set-nullable?}.

The abstract data type is encapsulated in the definitions
listed below:


@defun make-item g production right-hand-expansion
Return an item for production with the * before @var{right-hand-expansion}.
@var{right-hand-expansion} must be eq? to some tail of the expansion
part of @var{production}.
@end defun

@defun item-production g item
Return the production corresponding to @var{item}
@end defun

@defun item-position g item
Return the right-hand-expansion corresponding to @var{item}.
@end defun

@defun item-set g . items
Construct an item set from the item arguments.
@end defun

@defun item-set-map g f is
Call @var{f} repeatedly, passing each item in @var{is}, returning a list
of results.
@end defun

@defun pick-item-set-mappings g f is
Call @var{f} repeatedly, passing each item of @var{is}, returning a list
of the non-#f results.
@end defun

@defun item-set-union g isa isb isc ...
@defunx item-set-empty? g is
Standard set operations on item sets.
@end defun

@defun grammar-non-terminals g
Return the non-terminal symbols of @var{g}.

The list is computed by looking at the production symbol of each
production.
@end defun

@defun grammar-terminals g
Return the list of terminals of @var{g}.

The list is computed by looking at all the symbols
used in expansions, and subtracting out the non-terminals.
@end defun


@defun grammar-nullables g
Return those non-terminals of @var{g} that can derive the empty string.
@end defun


@node  Parsing,  , Grammars, Top
@chapter Parsing


This chapter is about how to parse using the kinds of grammar described
by the preceeding chapter.

@menu
* Calling @code{parser}::       
* Parsing Overview::            
* Parsing in Detail::           
@end menu

@node  Calling @code{parser}, Parsing Overview, Parsing, Parsing
@section Calling @code{parser}

@defun parser g lexer error module
Parse input according to the grammar @var{g}.

@var{lexer} should be a thunk that returns successive input tokens.

@var{error} is a procedure that is called in responce to parse
errors.   It is called with the arguments:

@example
(error continue-parser-loop state-stack value-stack token lexer)
@end example

The error routine can exit non-locally or return immediately (to the caller
of @code{parser}).  It can also invoke @code{continue-parser-loop} this way:

@example
(continue-parser-loop new-state-stack new-value-stack new-token new-lexer new-error)
@end example

to resume the parse.

@var{module} is the module in which to evaluate reduction procedures.
@end defun

These procedures operate on value stacks:

@defun parser-value-cons val stack
@defunx parser-value-car stack
@defunx parser-value-cdr stack
@defunx parser-value-cdr-ref stack
Standard stack operations for value stacks.
@end defun


These procedures operate on state stacks:

@defun parser-context-cons state stack
@defunx parser-context-car stack
@defunx parser-context-cdr stack
@defunx parser-context-cdr-ref stack
Standard stack operations for state stacks.
@end defun


@defun parser-start-state g
Return the parser starting state for grammar @var{g}.
@end defun

@defun make-parser-state g is
@defunx parser-state-item-set state
Return the parser state corresponding to an lr(0) item set (and vice versa).
@end defun

@defun parser-action g state token-type module
Return a parser action for @var{state} with a @var{token-type} look-ahead.

The return value is a list matching one of these patterns:

@example
;; A shift action.   The argument is the parser state after the shift.
;;
`(shift ,shift)

;; A reduce action.  The argument is a list of reduction procedures.
;;
`(reduce . ,funcs)

;; A parser error -- no action is defined.
;;
`(error)
@end example
@end defun

A reduce action is a procedure returned by @code{production-function}.

@defun production-function g production module
Return a reduction function for @var{production}.

The production contains expressions that are free in the variables
@code{$1}, @code{$2} etc.   @code{production-function} turns these
expressions into Scheme procedures.

Its probably not important to know how to call a reduction function --
this is something that only the parser itself should ever do.
@end defun

@node  Parsing Overview, Parsing in Detail, Calling @code{parser}, Parsing
@section Parsing Overview

The procedure @code{parser} implements a conventional shift-reduce
parser with one look-ahead token.  The parser has, however, some 
unsual properties:

@itemize @bullet{}
@item @emph{No reductions without shifting.}

Suppose the look-ahead can not be shifted, but that reductions (possilby
more than one) are possible.  @code{parser} will not perform any
reduction until it finds a series of reductions that will allow the
look-ahead token to be shifted.  If no such series of reductions exists,
than an error is signaled immediately.

A consequence of this property is that reduce/reduce conflicts in a
grammar are not necessarily bugs -- they may simply be points in a
grammar where the parser has to do a backtracking search through the
possible reductions to find a way to advance the parse.

@item @emph{Parsing Never Ends}

Parsing continues forever unless a reduction rule exits non-locally
(by invoking a continuation or @code{throw}).
@end itemize


@node  Parsing in Detail,  , Parsing Overview, Parsing
@section Parsing in Detail

The comments in lang/lr1.scm describe the parsing algorithm in greater
detail.

@bye
